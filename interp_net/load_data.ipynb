{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataset\n",
    "\n",
    "* y : (N,) discrete for classification, real values for regression\n",
    "* x : (N, D, tn) input multivariate time series data with dimension. \n",
    "  * N is number of data cases, D is the dimension of sparse and irregularly sampled time series and tn is the union of observed time stamps in all the dimension for a data case n. Since each tn is of variable length, we pad them with zeros to have an array representation.\n",
    "\n",
    "* m : (N, D, tn) where m[i,j,k] = 0 means that x[i,j,k] is not observed.\n",
    "* T : (N, D, tn) represents the actual time stamps of observation;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "with open('vitals_records.p', 'rb') as file:\n",
    "    vitals = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('adm_type_los_mortality.p', 'rb') as file:\n",
    "    adm_info = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm_id = [record[0] for record in adm_info]\n",
    "adm_id_needed = [record[0] for record in adm_info if record[2] >= 48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vitals_dict = {}\n",
    "for i in range(len(adm_id)):\n",
    "    vitals_dict[adm_id[i]] = vitals[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vitals = [vitals_dict[x] for x in adm_id_needed]\n",
    "label = [rec[3] for x in adm_id_needed for rec in adm_info if x == rec[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trim lossing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original code: https://github.com/mlds-lab/interp-net/blob/master/src/mimic_preprocessing.py#L25\n",
    "\n",
    "def trim_los(data, length_of_stay):\n",
    "    num_features = 12  # final features (excluding EtCO2)\n",
    "    max_length = 2881  # maximum length of time stamp\n",
    "    a = np.zeros((len(data), num_features, max_length))\n",
    "    timestamps = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        l = []\n",
    "        for elem in data[i][7]:\n",
    "            if elem[1] != None:\n",
    "                # Fahrenheit->Celcius conversion\n",
    "                tup = (elem[0], elem[1]*1.8 + 32)\n",
    "                data[i][6].append(tup)\n",
    "\n",
    "        for elem in data[i][10]:\n",
    "            data[i][9].append(elem)\n",
    "        for elem in data[i][11]:\n",
    "            data[i][9].append(elem)\n",
    "\n",
    "        # removing duplicates and EtCO2\n",
    "        del data[i][5]\n",
    "        del data[i][6]\n",
    "        del data[i][8]\n",
    "        del data[i][8]\n",
    "\n",
    "        # taking union of all time stamps,\n",
    "        # we don't actually need this for our model\n",
    "        for j in range(num_features):\n",
    "            for k in range(len(data[i][j])):\n",
    "                l.append(data[i][j][k][0])\n",
    "\n",
    "        # keeping only unique elements\n",
    "        TS = []\n",
    "        for j in l:\n",
    "            if j not in TS:\n",
    "                TS.append(j)\n",
    "        TS.sort()\n",
    "\n",
    "        # extracting first 48hr data\n",
    "        T = copy.deepcopy(TS)\n",
    "        TS = []\n",
    "        for t in T:\n",
    "            if (t - T[0]).total_seconds()/3600 <= length_of_stay:\n",
    "                TS.append(t)\n",
    "        T = []\n",
    "\n",
    "        \n",
    "        timestamps.append(TS)\n",
    "        for j in range(num_features):\n",
    "            c = 0\n",
    "            for k in range(len(TS)):\n",
    "                if c < len(data[i][j]) and TS[k] == data[i][j][c][0]:\n",
    "                    if data[i][j][c][1] is None:\n",
    "                        a[i, j, k] = -100  # missing data\n",
    "                    elif (data[i][j][c][1] == 'Normal <3 secs' or\n",
    "                          data[i][j][c][1] == 'Normal <3 Seconds' or\n",
    "                          data[i][j][c][1] == 'Brisk'):\n",
    "                        a[i, j, k] = 1\n",
    "                    elif (data[i][j][c][1] == 'Abnormal >3 secs' or\n",
    "                          data[i][j][c][1] == 'Abnormal >3 Seconds' or\n",
    "                          data[i][j][c][1] == 'Delayed'):\n",
    "                        a[i, j, k] = 2\n",
    "                    elif (data[i][j][c][1] == 'Other/Remarks' or\n",
    "                          data[i][j][c][1] == 'Comment'):\n",
    "                        a[i, j, k] = -100  # missing data\n",
    "                    else:\n",
    "                        a[i, j, k] = data[i][j][c][1]\n",
    "\n",
    "                    c += 1\n",
    "                else:\n",
    "                    a[i, j, k] = -100  # missing data\n",
    "\n",
    "    return a, timestamps\n",
    "\n",
    "hours_look_ahead = 48\n",
    "\n",
    "vitals, timestamps = trim_los(vitals, hours_look_ahead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixing input format\n",
    "\n",
    "Return the input in the proper format\n",
    "\n",
    "* x: observed values\n",
    "* M: masking, 0 indicates missing values\n",
    "* delta: time points of observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_input_format(x, T):\n",
    "    \"\"\"Return the input in the proper format\n",
    "    x: observed values\n",
    "    M: masking, 0 indicates missing values\n",
    "    delta: time points of observation\n",
    "    \"\"\"\n",
    "    timestamp = 200\n",
    "    num_features = 12\n",
    "\n",
    "    # trim time stamps higher than 200\n",
    "    for i in range(len(T)):\n",
    "        if len(T[i]) > timestamp:\n",
    "            T[i] = T[i][:timestamp]\n",
    "\n",
    "    x = x[:, :, :timestamp]\n",
    "    M = np.zeros_like(x)\n",
    "    delta = np.zeros_like(x)\n",
    "    print(x.shape, len(T))\n",
    "\n",
    "    for t in T:\n",
    "        for i in range(1, len(t)):\n",
    "            t[i] = (t[i] - t[0]).total_seconds()/3600.0\n",
    "        if len(t) != 0:\n",
    "            t[0] = 0\n",
    "\n",
    "    # count outliers and negative values as missing values\n",
    "    # M = 0 indicates missing value\n",
    "    # M = 1 indicates observed value\n",
    "    # now since we have mask variable, we don't need -100\n",
    "    M[x > 500] = 0\n",
    "    x[x > 500] = 0.0\n",
    "    M[x < 0] = 0\n",
    "    x[x < 0] = 0.0\n",
    "    M[x > 0] = 1\n",
    "\n",
    "    for i in range(num_features):\n",
    "        for j in range(x.shape[0]):\n",
    "            for k in range(len(T[j])):\n",
    "                delta[j, i, k] = T[j][k]\n",
    "\n",
    "    return x, M, delta\n",
    "\n",
    "\n",
    "x, M, delta = fix_input_format(vitals, timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean inputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_imputation(vitals, mask):\n",
    "    \"\"\"For the time series missing entirely, our interpolation network \n",
    "    assigns the starting point (time t=0) value of the time series to \n",
    "    the global mean before applying the two-layer interpolation network.\n",
    "    In such cases, the first interpolation layer just outputs the global\n",
    "    mean for that channel, but the second interpolation layer performs \n",
    "    a more meaningful interpolation using the learned correlations from\n",
    "    other channels.\"\"\"\n",
    "    counts = np.sum(np.sum(mask, axis=2), axis=0)\n",
    "    mean_values = np.sum(np.sum(vitals*mask, axis=2), axis=0)/counts\n",
    "    for i in range(mask.shape[0]):\n",
    "        for j in range(mask.shape[1]):\n",
    "            if np.sum(mask[i, j]) == 0:\n",
    "                mask[i, j, 0] = 1\n",
    "                vitals[i, j, 0] = mean_values[j]\n",
    "    return\n",
    "\n",
    "\n",
    "mean_imputation(x, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hold_out(mask, perc=0.2):\n",
    "    \"\"\"To implement the autoencoder component of the loss, we introduce a set\n",
    "    of masking variables mr (and mr1) for each data point. If drop_mask = 0,\n",
    "    then we removecthe data point as an input to the interpolation network,\n",
    "    and includecthe predicted value at this time point when assessing\n",
    "    the autoencoder loss. In practice, we randomly select 20% of the\n",
    "    observed data points to hold out from\n",
    "    every input time series.\"\"\"\n",
    "    drop_mask = np.ones_like(mask)\n",
    "    drop_mask *= mask\n",
    "    for i in range(mask.shape[0]):\n",
    "        for j in range(mask.shape[1]):\n",
    "            count = np.sum(mask[i, j], dtype='int')\n",
    "            if int(0.20*count) > 1:\n",
    "                index = 0\n",
    "                r = np.ones((count, 1))\n",
    "                b = np.random.choice(count, int(0.20*count), replace=False)\n",
    "                r[b] = 0\n",
    "                for k in range(mask.shape[2]):\n",
    "                    if mask[i, j, k] > 0:\n",
    "                        drop_mask[i, j, k] = r[index]\n",
    "                        index += 1\n",
    "    return drop_mask\n",
    "\n",
    "drop_mask=hold_out(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.concatenate((x, M, delta, drop_mask), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y= np.array(label)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('preprocessed_data.npz', array1=x, array2=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acknowledgement\n",
    "\n",
    "* https://github.com/mlds-lab/interp-net/blob/master/src/mimic_preprocessing.py#L25"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
