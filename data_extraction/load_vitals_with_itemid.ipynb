{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import psycopg2 as py\n",
    "\n",
    "np_data = np.load('list_adm_id.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_adm_id=np_data.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def split_list(lst, n):\n",
    "    chunk_size = math.ceil(len(lst) / n)\n",
    "    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of vitals with itemid sets\n",
    "vital_itemids = {\n",
    "    \"SpO2\": [646, 220277],\n",
    "    \"HR\": [211, 220045],\n",
    "    \"RR\": [618, 615, 220210, 224690],\n",
    "    \"SBP\": [51, 442, 455, 6701, 220179, 220050],\n",
    "    \"DBP\": [8368, 8440, 8441, 8555, 220180, 220051],\n",
    "    \"EtCO2\": [1817, 228640],\n",
    "    \"Temp_F\": [223761, 678],\n",
    "    \"Temp_C\": [223762, 676],\n",
    "    \"TGCS\": [198, 226755, 227013],\n",
    "    \"CRR\": [3348],\n",
    "    \"FiO2\": [2981, 3420, 3422, 223835],\n",
    "    \"Glucose\": [807, 811, 1529, 3745, 3744, 225664, 220621, 226537],\n",
    "    \"pH\": [780, 860, 1126, 1673, 3839, 4202, 4753, 6003, 220274, 220734, 223830, 228243],\n",
    "}\n",
    "\n",
    "# Urine output itemids as a separate set since it's fetched from outputevents\n",
    "urine_output_itemids = [\n",
    "    43647, 43053, 43171, 43173, 43333, 43347, 43348, 43355, 43365, \n",
    "    43373, 43374, 43379, 43380, 43431, 43519, 43522, 43537, 43576, \n",
    "    43583, 43589, 43638, 43654, 43811, 43812, 43856, 44706, 45304, 227519\n",
    "]\n",
    "\n",
    "# data = []\n",
    "# for id in range(len(list_adm_id)):\n",
    "#     hadm_id = list_adm_id[id][0]\n",
    "#     print(id, hadm_id)\n",
    "#     vitals = []\n",
    "\n",
    "#     # Loop through the vital itemids and execute a single query per vital type\n",
    "#     for vital_name, itemids in vital_itemids.items():\n",
    "#         itemid_str = ','.join(map(str, itemids))\n",
    "#         # cur.execute(f\"SELECT charttime, valuenum FROM chartevents WHERE hadm_id = %s AND itemid IN ({itemid_str}) ORDER BY charttime\", [hadm_id])\n",
    "#         vitals.append(f\"SELECT charttime, valuenum FROM chartevents WHERE hadm_id = {hadm_id} AND itemid IN ({itemid_str}) ORDER BY charttime\")\n",
    "\n",
    "#     # Fetch urine output from outputevents separately\n",
    "#     urine_itemid_str = ','.join(map(str, urine_output_itemids))\n",
    "#     # cur.execute(f\"SELECT charttime, VALUE FROM outputevents WHERE hadm_id = %s AND itemid IN ({urine_itemid_str}) ORDER BY charttime\", [hadm_id])\n",
    "#     vitals.append(f\"SELECT charttime, VALUE FROM outputevents WHERE hadm_id = {hadm_id} AND itemid IN ({urine_itemid_str}) ORDER BY charttime\")\n",
    "\n",
    "#     # Append the vitals for this admission ID to the main data list\n",
    "#     data.append(vitals)\n",
    "\n",
    "\n",
    "# Function to process each chunk and save it as a .npy file\n",
    "def process_chunk(chunk, chunk_index):\n",
    "    data = []\n",
    "    for id in range(len(chunk)):\n",
    "        hadm_id = chunk[id][0]\n",
    "        print(f\"Processing admission {id} in chunk {chunk_index}, hadm_id: {hadm_id}\")\n",
    "        vitals = []\n",
    "\n",
    "        # Loop through the vital itemids and execute a single query per vital type\n",
    "        for vital_name, itemids in vital_itemids.items():\n",
    "            itemid_str = ','.join(map(str, itemids))\n",
    "            # Here you would execute the query and fetch results\n",
    "            # cur.execute(f\"SELECT charttime, valuenum FROM chartevents WHERE hadm_id = %s AND itemid IN ({itemid_str}) ORDER BY charttime\", [hadm_id])\n",
    "            # For now, we are simulating the query\n",
    "            vitals.append(f\"SELECT charttime, valuenum FROM chartevents WHERE hadm_id = {hadm_id} AND itemid IN ({itemid_str}) ORDER BY charttime\")\n",
    "\n",
    "        # Fetch urine output from outputevents separately\n",
    "        urine_itemid_str = ','.join(map(str, urine_output_itemids))\n",
    "        # cur.execute(f\"SELECT charttime, VALUE FROM outputevents WHERE hadm_id = %s AND itemid IN ({urine_itemid_str}) ORDER BY charttime\", [hadm_id])\n",
    "        vitals.append(f\"SELECT charttime, VALUE FROM outputevents WHERE hadm_id = {hadm_id} AND itemid IN ({urine_itemid_str}) ORDER BY charttime\")\n",
    "\n",
    "        # Append the vitals for this admission ID to the main data list\n",
    "        data.append(vitals)\n",
    "\n",
    "    # Save the chunk's data as a .npy file\n",
    "    np.save(f\"chunk_{chunk_index}.npy\", np.array(data))\n",
    "    print(f\"Chunk {chunk_index} saved.\")\n",
    "\n",
    "\n",
    "chunks = split_list(list_adm_id, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded chunk_0.npy\n",
      "Loaded chunk_2.npy\n",
      "Loaded chunk_13.npy\n",
      "Loaded chunk_7.npy\n",
      "Loaded chunk_9.npy\n",
      "Loaded chunk_4.npy\n",
      "Loaded chunk_11.npy\n",
      "Loaded chunk_5.npy\n",
      "Loaded chunk_8.npy\n",
      "Loaded chunk_1.npy\n",
      "Loaded chunk_15.npy\n",
      "Loaded chunk_10.npy\n",
      "Loaded chunk_14.npy\n",
      "Loaded chunk_6.npy\n",
      "Loaded chunk_3.npy\n",
      "Loaded chunk_12.npy\n",
      "Combined data saved to combined_vitals_data.npy\n"
     ]
    }
   ],
   "source": [
    "def load_and_combine_npy_files(output_dir, combined_output_path):\n",
    "    \"\"\"\n",
    "    Load all .npy files from the output directory and combine them into a single file.\n",
    "    \"\"\"\n",
    "    combined_data = []\n",
    "    for filename in os.listdir(output_dir):\n",
    "        if filename.endswith('.npy'):\n",
    "            file_path = os.path.join(output_dir, filename)\n",
    "            chunk_data = np.load(file_path, allow_pickle=True)\n",
    "            combined_data.extend(chunk_data)\n",
    "            print(f\"Loaded {filename}\")\n",
    "    \n",
    "    # Save the combined data\n",
    "    np.save(combined_output_path, combined_data)\n",
    "    print(f\"Combined data saved to {combined_output_path}\")\n",
    "\n",
    "load_and_combine_npy_files('./vitals_with_item_id', 'combined_vitals_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File converted from .npy to .pkl\n"
     ]
    }
   ],
   "source": [
    "# Load the .npy file\n",
    "npy_data = np.load('combined_vitals_data.npy', allow_pickle=True)\n",
    "\n",
    "# Save it as a .pkl file\n",
    "with open('combined_vitals_data.pkl', 'wb') as pkl_file:\n",
    "    pickle.dump(npy_data, pkl_file)\n",
    "\n",
    "print(\"File converted from .npy to .pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
